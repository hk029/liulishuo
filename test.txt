What's wrong with AI? Try asking a human being 
人工智能怎么了？试着去问问人类吧
Amazon has apparently abandoned an AI system aimed at automating its recruitment process. The system gave job candidates scores ranging from one to five stars, a bit like shoppers rating products on the Amazon website.
亚马逊似乎已经放弃了旨在使其招聘过程自动化的一个人工智能系统。该系统给求职者打分，从一星到五星不等，有点类似于顾客在亚马逊网站上给产品打分。
The trouble was, the program tended to give five stars to men and one star to women. According to Reuters, it “penalised résumés that included the word ‘women’s’, as in ‘women’s chess club captain’” and marked down applicants who had attended women-only colleges.
问题在于，该程序往往给男性求职者打五星，给女性求职者却只打一星。据路透社报道，该系统“将含有‘女性的’这几个字的简历置于不利地位，比如写有‘女子象棋俱乐部队长’这样的简历就会失分”，除此之外，该系统还压低了上过女子大学的申请人的分数。
It wasn’t that the programme was malevolently misogynistic. Rather, like all AI programs, it had to be “trained” by being fed data about what constituted good results. Amazon, naturally, fed it with details of its own recruitment programme over the previous 10 years. Most applicants had been men, as had most recruits. What the program learned was that men, not women, were good candidates.
这并不是说这个程序患有恶毒的厌女症。相反，正如所有人工智能程序一样，它必定被“训练”过，训练方式是向其灌输数据，这些数据表明什么可以被视为好结果。亚马逊自然也向它灌输了过去 10 年里招聘方案的细节信息。大多数申请者都是男性，大多数新员工也都是男性。这个程序学到的是：男人，而不是女人，才是好的候选人。
It’s not the first time AI programs have been shown to exhibit bias. Software used in the US justice system to assess a criminal defendant’s likelihood of reoffending is more likely to judge black defendants as potential recidivists. Facial recognition software is poor at recognising non-white faces. A Google photo app even labelled African Americans “gorillas”.
这已经不是人工智能程序第一次显示出偏见了。美国司法系统用来评估刑事被告再次犯罪的可能性的软件，更有可能将黑人被告视为潜在的累犯。人脸识别软件对非白种人的面部识别能力较差。而谷歌的一个图片应用程序甚至把非裔美国人标注为“大猩猩”。
All this should teach us three things. First, the issue here is not to do with AI itself, but with social practices. The biases are in real life.
所有这一切都应该告诉我们三件事。首先，这里的问题不是与人工智能本身有关，而是与社会实践有关。这些偏见存在于现实生活中。
Second, the problem with AI arises when we think of machines as being objective. A machine is only as good as the humans programming it.
第二，当我们认为机器是客观的时候，人工智能的问题出现了。机器如何表现，仅取决于对它编程的人们。
And third, while there are many circumstances in which machines are better, especially where speed is paramount, we have a sense of right and wrong and social means of challenging bias and injustice. We should never deprecate that.
第三，在许多情况下，机器表现更好，特别是在速度至上的情况下，但我们有对正确和错误的识别能力，以及挑战偏见和不公正的社会手段。我们永远不应该忽视这一点。
—————  文章来源 / 卫报 
重点词汇
automate/ˈɔːtəmeɪt/
v. 自动化
e.g.
use computers to automate the process
penalise/ˈpiːnəlaɪz/
v. 置于不利地位；不公正对待
e.g.
The law appears to penalise the poorest members of society.
résumé/ˈrezjumeɪ/
n. 简历
mark down //
降低......的分数
e.g.
She was marked down because of poor grammar.
malevolently/məˈlevələntli/
adv. 恶意地
e.g.
benevolent
misogynistic/mɪˌsɑːdʒɪˈnɪstɪk/ 
adj. 厌女的
e.g.
misogynist
recruit/rɪˈkruːt/
n. 新成员，新人
likelihood/ˈlaɪklihʊd/ 
n. 可能性
e.g.
The likelihood of infection
reoffend/ˌriːəˈfend/
v. 再次犯罪，再次犯法
e.g.
Without help, many released prisoners will reoffend.
recidivist/rɪˈsɪdɪvɪst/
n. 惯犯，累犯
paramount/ˈpærəmaʊnt/ 
adj. 最高的，至上的
deprecate/ˈdeprəkeɪt/
v. 反对；忽视，看不起
e.g.
He always deprecates my achievements.
拓展内容
人工智能的“种族主义”


2010 年一个秋天的早晨，萨菲亚·乌莫加·诺布尔（Safiya Umoja Noble）坐在伊利诺伊州家中的餐桌前，在谷歌上搜索词汇。

当时她正准备与她十四岁的继女和五个小侄女一起过夜。“我想搜一下‘黑人女孩’，因为我脑子里想的是她们——我最喜欢的黑人女孩组合，”她说。

但看似无害的搜索引擎显示的结果却令人震惊：一个充满了色情链接的页面。那时候，任何人在谷歌搜索“黑人女孩”，都会得到同样的结果。

“太令人失望了，几乎所有将种族和女孩联系在一起的搜索结果都以色情内容为主。” 南加州大学的传播学教授的诺布尔说，“我赶紧把电脑拿开了，希望孩子们都不会提出要玩它。”

在美国的另外一座城市，计算机科学家乔伊·布兰威尼（Joy Buolamwini）发现了 AI 的另外一个代表性问题。乔伊出生在加拿大，父母来自加纳，她发现当时先进的人脸识别系统，例如 IBM 和 Microsoft 使用的人脸识别系统并不能识别她深色的皮肤。有时更糟，程序根本发现不了她正站在前方。当时她是乔治亚理工学院的一名学生，正在研究一个机器人项目，却发现本应识别并躲避用户的机器人根本无法认出她来。她只好求助浅肤色的室友完成了这个项目。

几年后，作为麻省理工大学的研究生，乔伊发现最新的电脑软件还是看不到她。但当她戴上白色面具——那种可以在沃尔玛买到的万圣节新奇道具，识别就很顺利。于是她戴着那个面具完成了项目编程。
20:16	
