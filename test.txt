How people think that self-driving cars should behave in an accident?
自动驾驶汽车在事故中应该保护谁？
In a paper just published in Nature, a team of psychologists and computer scientists describe a different approach. They created the “Moral Machine”, a website which presents visitors with a series of choices about whom to save and whom to kill.
在最近发表于《自然》杂志上的一篇论文中，一支由心理学家和计算机科学家组成的团队描述了另一种方法。他们创造了“道德机器”网站。这个网站会给访客展示一系列的选择，从而决定该救谁，该杀谁。
The strongest preferences, expressed by respondents from all over the world, were for saving human lives over animal ones, preferring to save many rather than few and prioritising children over the old. There were weaker preferences for saving women over men, pedestrians over passengers in the car and for taking action rather than doing nothing. Criminals were seen as literally subhuman—ranking below dogs in the public’s priority list, but above cats. It is easy to imagine the utilitarian argument for preserving the lives of doctors over others. Humanity’s (weak) preference for saving athletes seems less intuitive.
全世界的答卷人展现出的最明显的倾向是，优先救人类而不是动物的性命，优先救多数而不是少数，优先救孩子而不是老人。以下几种倾向则没有那么明显：救女性而非男性，行人而非车里的乘客，采取行动而非听其自然。罪犯真正意义上地被视为低人一等，在公众的优先级名单上排序低于狗，但是高于猫。优先救医生而非其他人的性命，这背后功利主义的考量并不难想象。但人们（较弱的）优先救运动员的倾向，看起来就没那么容易理解了。
Many people, says Dr Rahwan, a computer scientist at MIT and one of the paper’s authors, dismiss the trolley problem as a piece of pointless hypothesising that is vanishingly unlikely to arise in real life. He is unconvinced. The specific situations posed by the website may hardly ever occur, he says. But all sorts of choices made by the firms producing self-driving cars will affect who lives and who dies in indirect, statistical ways. He gives the example of overtaking cyclists: “If you stay relatively near to the cycle lane, you’re increasing the chance of hitting a cyclist, but reducing the chance of hitting another car in the next lane over,” he says. “Repeat that over hundreds of millions of trips, and you’re going to see a skew in the [accident] statistics.”
拉万博士是麻省理工的计算机科学家，也是这篇论文的作者之一，他说许多人不再思考电车问题，将它视为一个没有意义的假说，认为它几乎不可能在现实中出现。拉万并没有被这种看法说服。他说，网站上给出的具体场景可能几乎不会出现，但是生产自动驾驶汽车的公司做的各种各样的抉择会影响到孰生孰死，这种影响是间接的，统计学层面的。他拿赶超骑行者一事举了例子：“如果你相对离自行车道近一些，你就增加了撞到骑行者的几率，但是降低了和旁边车道的另一辆汽车相撞的几率。”他说，“在上亿次旅行中重复这一情况，你就发现事故统计数据的偏态。”
—————  文章来源 / 经济学人 
重点词汇
moral/ˈmɔːrəl/
adj. 道德的
n. 道德
e.g.
a moral judgement
moral codes
preference/ˈprefrəns/
n. 偏爱；偏爱的事物；优先选择权
e.g.
a preference for seafood over steak
choose to eat seafood in preference to steak
prefer to ... rather than ...//
宁愿做某事而不做另一件事
e.g.
prefer to sleep rather than swim
prioritise ... over ... //
优先考虑 A 而不是 B
pedestrian/pəˈdestriən/
n. 步行者，行人
literally/ˈlɪtərəli/
adv. 字面意义地；（夸张）简直
e.g.
His brain was literally turned off.
I have made literally thousands of phone calls.
subhuman/ˌsʌbˈhjuːmən/
adj. 不齿于人类的；低于人类的
e.g.
Parents regard all game junkies as subhuman.
utilitarian/ˌjuːtɪlɪˈteriən/
adj. 功利主义的
intuitive/ɪnˈtuːɪtɪv/
adj. 容易理解的
dismiss/dɪsˈmɪs/
v. 把…打发走；不再考虑
the trolley problem//
电车问题
hypothesise/haɪˈpɑːθəsaɪz/
v. 假设，假定
vanishingly/ˈvænɪʃɪŋlɪ/
adv. 可以忽略地
skew/skjuː/
n. （统计）偏态，偏斜性
拓展内容
Moral Machine 官方网站：
http://moralmachine.mit.edu/
有轨电车难题以及现实中的应对方法


1967 年，菲力帕·芙特首次提出了“电车难题”。这个思想实验，可以追溯到伯纳德·威廉姆斯提出的枪决原住民问题：假设一位植物学家，有天到一个独裁国家中游玩。当地独裁者逮捕了 20 名无辜的印地安人，以涉嫌叛乱的罪名，将他们全部判处死刑。但是这个独裁者提出一个建议：身为客人，如果这个植物学家亲手枪决其中一个印地安人，其他 19 个人就可以因此被释放。这个植物学家是否应该亲自枪决一位，以拯救其余 19 人，还是拒绝动手，坐视这 20 个人都被枪决？

类似的思想实验主要想探讨功利主义和源自康德主义的道德义务论。功利主义认为，为追求对大多数人来说的最大效益，应该牺牲少数人来拯救多数人。康德主义认为，道德应该建立在必要的义务责任上。如果不可以杀人是一种道德义务，就不可以动手让一个人牺牲，即使这个行为的后果是牺牲更多人。

在现实中，遇到类似情况时，司机该做的不是转向，而是刹车。如果刹车失灵是机器故障或维修员失误，而不是司机疏于检查，那对司机来讲是不可抗力。司机尽力找方法刹车仍然没能避免事故的话，司机可以主张自己没有过失，从而避免刑事责任。但是如果司机主动转向而撞死人，就要承担事故产生的过失杀人罪的刑事责任。类似的状况是恐怖分子的要胁。如果顺从恐怖分子会死比较少的人，不顺从会死比较多的人，通常政府不会计算哪一边死的人少，而是拒绝谈判。
00:00	
